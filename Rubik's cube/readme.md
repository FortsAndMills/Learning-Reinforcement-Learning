## Текущая версия
* Double DQN: с учётом детерминированности среды превращается в из Q-learning-а в V-learning
* хубер-лосс вместо обычного квадратичного: дополнительная стабилизация лосса
* кастомная стратегия exploration-а: вычитаем из Q(s, a) максимальное по a значение, прибавляем единицу. Убираем все действия, для которых это значение меньше 0. Оставшиеся - нормируем, превращая в распределение, из которого и сэмплируем. Если Value-функция окажется истиной, все неправильные действия в такой стратегии будут откинуты.
* модель нейросети, по состоянию (конфигурации КР) выдающей количество ходов до собранной конфигурации, то есть Value данной конфигурации: принимает на вход представление из 48 категориальных признаков (цвета от 1 до 6), закодированных one_hot, преобразует его в сто фич. Применяем эту модель ко всем 24 инвариантным представлениям текущей конфигурации (смотрим на КР под всеми углами со всех сторон), берём максимум, затем преобразуем ещё небольшим полносвязником 100 фич к ответу.

## Текущие результаты:
Первые 4 уровня проходит (т.е. проходит с качеством, выше 95%). На 5 уровне можно добить качество также до 95%, подождав более 2000 игр. На 6-ом уровне после 20 000 игр (ночь...) качество добить выше 0.86 не удалось (что означает, что на следующих уровнях качество и дальше будет падать).

Если давать только 2000 игр на каждый уровень, то:
* 6 уровень - 0.73
* 7 уровень - 0.56
* 8 уровень - 0.59
* 9 уровень - 0.38
* 10 уровень - 0.23

А википедия подсказывает, что нужно уметь проходить 26 уровней.

## Navigating Rubik:
Попытка посмотреть на задачу под другим углом. Отчасти, основная проблема предыдущего подхода - при увеличении сложности сеть получает на вход состояния, с которыми раньше принципиально не работала. Попробуем убрать этот костыль. Стартуем в рандомных (абсолютно) состояниях и пытаемся достичь собранного. Возникает проблема, что ненулевой награды никогда не будет. Решение - HER: будем искать путь в заданное состояние.

Итого: состояние - это две конфигурации кубика-рубика, действия меняют только одну конфигурацию и пытаются свести ко второй. Задача посложнее, чем исходная, но у этой задачи уже есть некостыльные решения.

Было сделано:
1) не поленился и написал реализацию КР на торче с параллельной батчевой обработкой многих кубиков-рубиков одновременно (я смог запихать 512 на граф.карту!!!).
2) в силу специфики среды и благодаря новой реализации всю обработку перенёс на граф.карту, включая experience replay
3) написал HER, который добавляет в experience не только пройденную траекторию, но и встречающиеся траектории длины 1...k
4) заготовил k "тестовых" КР, где целевое состояние отстоит от начального на 1 ... k шагов, каждые несколько итераций обучения тестируем сетку на них.

Пробовал k = 2, 3, 4... Не работало: так, при k = 2 нейронка выдаёт V(s)=-2 у конфигурации, которую можно получить, сделав правильное действие, и чушь у остальных соседних конфигураций => первое действие не очень правильное. Увеличение k чуть-чуть помогает...

Дальше интересный результат: я тупо запустил код при k = 26. И... сеть стала обучаться. Оставил на сутки. Начала решать почти 100% с расстояния 2, чему-то научилась в том числе на расстоянии 10, но по сравнению с предыдущими экспериментами результат хуже (в предыдущих экспериментах можно было добиться ~95% на уровне 5). Видно по кривым, что качество тихоооооонечко растёт, но игры с learning rate и даже оптимизатором (пробовал сам Янушеоптимизатор!!).

И тут обнаружился забавный момент: выяснилось, что если на каждом шаге в 512 КР генерится по 26 траекторий для experience replay, то (на соседних же шагах траектории очень похоже) 26 шагов занимают ТРЕТЬ объёма памяти (1 миллион транзишнов). Ну, то есть вообще говоря, на каждом шаге в нейросеть подаётся батч из относительно похожих конфигураций КР. Проблему решить можно просто: сохраняя каждый транзишн с некоторой вероятностью (скажем, 5%). Привело к падению качества О_О. Кажется, если батч состоит из очень непохожих конфигураций, сеть начинает отделять КР из одной точки пространства от других... Такие дела.

### A2C / GAE:

Policy Gradient-алгоритмам тут, кажется, совсем плохо становится.
