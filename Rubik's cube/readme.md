## Текущая версия
* Double DQN: с учётом детерминированности среды превращается в из Q-learning-а в V-learning
* хубер-лосс вместо обычного квадратичного: дополнительная стабилизация лосса
* кастомная стратегия exploration-а: вычитаем из Q(s, a) максимальное по a значение, прибавляем единицу. Убираем все действия, для которых это значение меньше 0. Оставшиеся - нормируем, превращая в распределение, из которого и сэмплируем. Если Value-функция окажется истиной, все неправильные действия в такой стратегии будут откинуты.
* модель нейросети, по состоянию (конфигурации КР) выдающей количество ходов до собранной конфигурации, то есть Value данной конфигурации: принимает на вход представление из 48 категориальных признаков (цвета от 1 до 6), закодированных one_hot, преобразует его в сто фич. Применяем эту модель ко всем 24 инвариантным представлениям текущей конфигурации (смотрим на КР под всеми углами со всех сторон), берём максимум, затем преобразуем ещё небольшим полносвязником 100 фич к ответу.

## Текущие результаты:
Первые 4 уровня проходит (т.е. проходит с качеством, выше 95%). На 5 уровне можно добить качество также до 95%, подождав более 2000 игр. На 6-ом уровне после 20 000 игр (ночь...) качество добить выше 0.86 не удалось (что означает, что на следующих уровнях качество и дальше будет падать).

Если давать только 2000 игр на каждый уровень, то:
* 6 уровень - 0.73
* 7 уровень - 0.56
* 8 уровень - 0.59
* 9 уровень - 0.38
* 10 уровень - 0.23

А википедия подсказывает, что нужно уметь проходить 26 уровней.
